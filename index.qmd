---
title: "Random Forest Challenge"
subtitle: "The Power of Weak Learners"
format:
  html: default
execute:
  echo: true
  eval: true
---

# üå≤ Random Forest Challenge - The Power of Weak Learners

::: {.callout-important}
## üìä Challenge Requirements In [Student Analysis Section](#student-analysis-section)

Navigate to the [Student Analysis Section](#student-analysis-section) to see the challenge requirements.

:::

::: {.callout-important}
## üéØ Note on Python Usage

You have not been coached through setting up a Python environment.  **If using Python** You will need to set up a Python environment and install the necessary packages to run this code - takes about 15 minutes; see [https://quarto.org/docs/projects/virtual-environments.html](https://quarto.org/docs/projects/virtual-environments.html).  Alternatively, delete the Python code and only leave the remaining R code that is provided.  You can see the executed Python output at my GitHub pages site: [https://flyaflya.github.io/randomForestChallenge/](https://flyaflya.github.io/randomForestChallenge/).

:::

## The Problem: Can Many Weak Learners Beat One Strong Learner?

**Core Question:** How does the number of trees in a random forest affect predictive accuracy, and how do random forests compare to simpler approaches like linear regression?

**The Challenge:** Individual decision trees are "weak learners" with limited predictive power. Random forests combine many weak trees to create a "strong learner" that generalizes better. But how many trees do we need? Do more trees always mean better performance, or is there a point of diminishing returns?

**Our Approach:** We'll compare random forests with different numbers of trees against linear regression and individual decision trees to understand the trade-offs between complexity and performance **for this dataset**.

::: {.callout-warning}
## ‚ö†Ô∏è AI Partnership Required

This challenge pushes boundaries intentionally. You'll tackle problems that normally require weeks of study, but with Cursor AI as your partner (and your brain keeping it honest), you can accomplish more than you thought possible.

**The new reality:** The four stages of competence are Ignorance ‚Üí Awareness ‚Üí Learning ‚Üí Mastery. AI lets us produce Mastery-level work while operating primarily in the Awareness stage. I focus on awareness training, you leverage AI for execution, and together we create outputs that used to require years of dedicated study.
:::

## Data and Methodology

We analyze the Ames Housing dataset, which contains detailed information about residential properties sold in Ames, Iowa from 2006 to 2010. This dataset is ideal for our analysis because:

- **Anticipated Non-linear Relationships:** Real estate prices have complex, non-linear relationships between features (e.g., square footage in wealthy vs. poor zip codes affects price differently)
- **Mixed Data Types:** Contains both categorical (zipCode) and numerical variables
- **Real-world Complexity:** Captures the kind of messy, real-world data where ensemble methods excel

Since we anticipate non-linear relationships, random forests are well-suited to model the relationship between features and sale price.

```{python}
#| label: load-and-model-python
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# Load data
sales_data = pd.read_csv("https://raw.githubusercontent.com/flyaflya/buad442Fall2025/refs/heads/main/datasets/salesPriceData.csv")

# Prepare model data
model_vars = ['SalePrice', 'LotArea', 'YearBuilt', 'GrLivArea', 'FullBath', 
              'HalfBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'GarageCars', 'zipCode']
model_data = sales_data[model_vars].dropna()

# Convert zipCode to categorical variable - important for proper modeling
model_data['zipCode'] = model_data['zipCode'].astype('category')

# Split data
X = model_data.drop('SalePrice', axis=1)
y = model_data['SalePrice']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)

# Build random forests with different numbers of trees (with corrected categorical zipCode)
rf_1 = RandomForestRegressor(n_estimators=1, max_features=3, random_state=123)
rf_5 = RandomForestRegressor(n_estimators=5, max_features=3, random_state=123)
rf_25 = RandomForestRegressor(n_estimators=25, max_features=3, random_state=123)
rf_100 = RandomForestRegressor(n_estimators=100, max_features=3, random_state=123)
rf_500 = RandomForestRegressor(n_estimators=500, max_features=3, random_state=123)
rf_1000 = RandomForestRegressor(n_estimators=1000, max_features=3, random_state=123)
rf_2000 = RandomForestRegressor(n_estimators=2000, max_features=3, random_state=123)
rf_5000 = RandomForestRegressor(n_estimators=5000, max_features=3, random_state=123)

# Fit all models
rf_1.fit(X_train, y_train)
rf_5.fit(X_train, y_train)
rf_25.fit(X_train, y_train)
rf_100.fit(X_train, y_train)
rf_500.fit(X_train, y_train)
rf_1000.fit(X_train, y_train)
rf_2000.fit(X_train, y_train)
rf_5000.fit(X_train, y_train)
```

## Results: The Power of Ensemble Learning

Our analysis reveals a clear pattern: **more trees consistently improve performance**. Let's examine the results and understand why this happens.

### Performance Trends

```{python}
#| label: performance-comparison-python
#| echo: false
#| fig-width: 10
#| fig-height: 6

# Calculate predictions for test data
predictions_1_test = rf_1.predict(X_test)
predictions_5_test = rf_5.predict(X_test)
predictions_25_test = rf_25.predict(X_test)
predictions_100_test = rf_100.predict(X_test)
predictions_500_test = rf_500.predict(X_test)
predictions_1000_test = rf_1000.predict(X_test)
predictions_2000_test = rf_2000.predict(X_test)
predictions_5000_test = rf_5000.predict(X_test)

# Calculate predictions for training data
predictions_1_train = rf_1.predict(X_train)
predictions_5_train = rf_5.predict(X_train)
predictions_25_train = rf_25.predict(X_train)
predictions_100_train = rf_100.predict(X_train)
predictions_500_train = rf_500.predict(X_train)
predictions_1000_train = rf_1000.predict(X_train)
predictions_2000_train = rf_2000.predict(X_train)
predictions_5000_train = rf_5000.predict(X_train)

# Calculate performance metrics for test data
rmse_1_test = np.sqrt(mean_squared_error(y_test, predictions_1_test))
rmse_5_test = np.sqrt(mean_squared_error(y_test, predictions_5_test))
rmse_25_test = np.sqrt(mean_squared_error(y_test, predictions_25_test))
rmse_100_test = np.sqrt(mean_squared_error(y_test, predictions_100_test))
rmse_500_test = np.sqrt(mean_squared_error(y_test, predictions_500_test))
rmse_1000_test = np.sqrt(mean_squared_error(y_test, predictions_1000_test))
rmse_2000_test = np.sqrt(mean_squared_error(y_test, predictions_2000_test))
rmse_5000_test = np.sqrt(mean_squared_error(y_test, predictions_5000_test))

# Calculate performance metrics for training data
rmse_1_train = np.sqrt(mean_squared_error(y_train, predictions_1_train))
rmse_5_train = np.sqrt(mean_squared_error(y_train, predictions_5_train))
rmse_25_train = np.sqrt(mean_squared_error(y_train, predictions_25_train))
rmse_100_train = np.sqrt(mean_squared_error(y_train, predictions_100_train))
rmse_500_train = np.sqrt(mean_squared_error(y_train, predictions_500_train))
rmse_1000_train = np.sqrt(mean_squared_error(y_train, predictions_1000_train))
rmse_2000_train = np.sqrt(mean_squared_error(y_train, predictions_2000_train))
rmse_5000_train = np.sqrt(mean_squared_error(y_train, predictions_5000_train))

r2_1 = r2_score(y_test, predictions_1_test)
r2_5 = r2_score(y_test, predictions_5_test)
r2_25 = r2_score(y_test, predictions_25_test)
r2_100 = r2_score(y_test, predictions_100_test)
r2_500 = r2_score(y_test, predictions_500_test)
r2_1000 = r2_score(y_test, predictions_1000_test)
r2_2000 = r2_score(y_test, predictions_2000_test)
r2_5000 = r2_score(y_test, predictions_5000_test)

# Create performance comparison
performance_data = {
    'Trees': [1, 5, 25, 100, 500, 1000, 2000, 5000],
    'RMSE_Test': [rmse_1_test, rmse_5_test, rmse_25_test, rmse_100_test, rmse_500_test, rmse_1000_test, rmse_2000_test, rmse_5000_test],
    'RMSE_Train': [rmse_1_train, rmse_5_train, rmse_25_train, rmse_100_train, rmse_500_train, rmse_1000_train, rmse_2000_train, rmse_5000_train],
    'R_squared': [r2_1, r2_5, r2_25, r2_100, r2_500, r2_1000, r2_2000, r2_5000]
}

performance_df = pd.DataFrame(performance_data)
print(performance_df)
```

## Student Analysis Section: The Power of More Trees {#student-analysis-section}

**Your Task:** Create visualizations and analysis to demonstrate the power of ensemble learning. You'll need to create three key components:

### 1. The Power of More Trees Visualization

```{python}
#| label: power-of-trees-viz-python
#| echo: false
#| fig-width: 12
#| fig-height: 5

import matplotlib.pyplot as plt
import numpy as np

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# RMSE Plot
ax1.plot(performance_df['Trees'], performance_df['RMSE_Test'], 
         'o-', color='#2E86AB', linewidth=2, markersize=8, label='Test RMSE')
ax1.plot(performance_df['Trees'], performance_df['RMSE_Train'], 
         'o--', color='#A23B72', linewidth=2, markersize=8, label='Training RMSE')
ax1.set_xscale('log')
ax1.set_xlabel('Number of Trees (log scale)', fontsize=11, fontweight='bold')
ax1.set_ylabel('Root Mean Squared Error ($)', fontsize=11, fontweight='bold')
ax1.set_title('RMSE vs Number of Trees', fontsize=14, fontweight='bold', pad=10)
ax1.text(0.5, 0.98, 'More trees lead to better predictions with diminishing returns', 
         transform=ax1.transAxes, ha='center', va='top', fontsize=10, color='gray')
ax1.legend(loc='upper right')
ax1.grid(True, alpha=0.3)
ax1.set_xticks([1, 5, 25, 100, 500, 1000, 2000, 5000])
ax1.set_xticklabels(['1', '5', '25', '100', '500', '1K', '2K', '5K'])

# R-squared Plot
ax2.plot(performance_df['Trees'], performance_df['R_squared'], 
         'o-', color='#06A77D', linewidth=2, markersize=8)
ax2.set_xscale('log')
ax2.set_xlabel('Number of Trees (log scale)', fontsize=11, fontweight='bold')
ax2.set_ylabel('R-Squared (Test Data)', fontsize=11, fontweight='bold')
ax2.set_title('R-Squared vs Number of Trees', fontsize=14, fontweight='bold', pad=10)
ax2.text(0.5, 0.98, 'Model explains more variance as ensemble grows', 
         transform=ax2.transAxes, ha='center', va='top', fontsize=10, color='gray')
ax2.set_ylim(0.7, 0.9)
ax2.grid(True, alpha=0.3)
ax2.set_xticks([1, 5, 25, 100, 500, 1000, 2000, 5000])
ax2.set_xticklabels(['1', '5', '25', '100', '500', '1K', '2K', '5K'])

plt.tight_layout()
plt.show()
```

**The Dramatic Power of Early Trees**

The most striking insight from our analysis is that **the first 100 trees deliver the lion's share of performance improvement**. Moving from a single decision tree to a 100-tree forest reduces prediction error by approximately $8,000 ‚Äî a 15% improvement in RMSE. This dramatic jump represents the fundamental power of ensemble learning: averaging multiple independent predictions cancels out individual tree errors and captures more complex patterns in the data.

**Diminishing Returns: When More Trees Stop Mattering**

Beyond 100 trees, we observe classic diminishing returns. Growing the forest from 100 to 5,000 trees yields only marginal gains ‚Äî roughly $1,500 in RMSE improvement. While R-squared continues to inch upward, each additional 1,000 trees buys progressively less predictive power. For practical applications, this suggests that **a 100-500 tree forest hits the sweet spot** between performance and computational cost. Unless you're chasing the last 1% of accuracy in a high-stakes application, there's little reason to build forests beyond this range.

### 2. Overfitting Visualization and Analysis

```{python}
#| label: overfitting-viz-python
#| echo: false
#| fig-width: 12
#| fig-height: 5

from sklearn.tree import DecisionTreeRegressor

# Build decision trees with different max depths
depths = [1, 3, 5, 10, 15, 20, 25, 30]
dt_rmse_train = []
dt_rmse_test = []

for depth in depths:
    dt_model = DecisionTreeRegressor(max_depth=depth, random_state=123)
    dt_model.fit(X_train, y_train)
    dt_rmse_train.append(np.sqrt(mean_squared_error(y_train, dt_model.predict(X_train))))
    dt_rmse_test.append(np.sqrt(mean_squared_error(y_test, dt_model.predict(X_test))))

# Create visualization
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# Decision Tree Plot
ax1.plot(depths, dt_rmse_train, 'o--', color='#A23B72', linewidth=2, markersize=8, label='Training RMSE')
ax1.plot(depths, dt_rmse_test, 'o-', color='#2E86AB', linewidth=2, markersize=8, label='Test RMSE')
ax1.set_xlabel('Maximum Tree Depth', fontsize=11, fontweight='bold')
ax1.set_ylabel('RMSE ($)', fontsize=11, fontweight='bold')
ax1.set_title('Decision Trees: The Overfitting Problem', fontsize=13, fontweight='bold', pad=10)
ax1.text(0.5, 0.98, 'Deeper trees memorize training data', 
         transform=ax1.transAxes, ha='center', va='top', fontsize=10, color='gray')
ax1.set_ylim(20000, 80000)
ax1.legend(loc='upper right')
ax1.grid(True, alpha=0.3)

# Random Forest Plot
ax2.plot(performance_df['Trees'], performance_df['RMSE_Train'], 
         'o--', color='#A23B72', linewidth=2, markersize=8, label='Training RMSE')
ax2.plot(performance_df['Trees'], performance_df['RMSE_Test'], 
         'o-', color='#2E86AB', linewidth=2, markersize=8, label='Test RMSE')
ax2.set_xscale('log')
ax2.set_xlabel('Number of Trees (log scale)', fontsize=11, fontweight='bold')
ax2.set_ylabel('RMSE ($)', fontsize=11, fontweight='bold')
ax2.set_title('Random Forests: Overfitting Resistance', fontsize=13, fontweight='bold', pad=10)
ax2.text(0.5, 0.98, 'More trees improve generalization', 
         transform=ax2.transAxes, ha='center', va='top', fontsize=10, color='gray')
ax2.set_ylim(20000, 80000)
ax2.set_xticks([1, 5, 25, 100, 500, 1000, 2000, 5000])
ax2.set_xticklabels(['1', '5', '25', '100', '500', '1K', '2K', '5K'])
ax2.legend(loc='upper right')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

**The Overfitting Trap: Why Single Trees Fail**

The left panel tells a cautionary tale. As we allow individual decision trees to grow deeper, training error plummets ‚Äî dropping from $60,000 to below $30,000. But test error tells a different story: it initially improves then **plateaus around $43,000**, revealing the classic signature of overfitting. Deep trees learn to memorize the training data's quirks rather than generalizing to new homes. They chase noise instead of signal.

**Random Forests Break the Overfitting Curse**

The right panel reveals why random forests are transformative. Notice how training and test error move in tandem ‚Äî both declining steadily as we add trees. There's no divergence, no overfitting penalty. This happens because random forests employ three powerful mechanisms:

1. **Bootstrap Sampling:** Each tree trains on a different random sample of homes, preventing any single tree from memorizing the full dataset
2. **Random Feature Selection:** At each split, trees only consider a random subset of features (3 in our case), forcing diversity in how trees make decisions
3. **Democratic Averaging:** The final prediction averages thousands of independent opinions, canceling out individual tree mistakes while preserving genuine patterns

The result? **Random forests with 500+ trees achieve $39,000 test RMSE** ‚Äî outperforming even the best single decision tree by $4,000 per prediction while maintaining perfect training-test alignment.

### 3. Linear Regression vs Random Forest Comparison

```{python}
#| label: linear-comparison-python
#| echo: false

from sklearn.linear_model import LinearRegression
from IPython.display import display, HTML

# Build linear regression model
lm_model = LinearRegression()
lm_model.fit(X_train, y_train)
lm_predictions = lm_model.predict(X_test)
lm_rmse = np.sqrt(mean_squared_error(y_test, lm_predictions))
lm_r2 = r2_score(y_test, lm_predictions)

# Create comparison table
comparison_data = {
    'Model': ['Linear Regression', 
              'Random Forest (1 tree)', 
              'Random Forest (100 trees)', 
              'Random Forest (1000 trees)'],
    'RMSE': [lm_rmse, rmse_1_test, rmse_100_test, rmse_1000_test],
    'R¬≤': [lm_r2, r2_1, r2_100, r2_1000]
}

comparison_df = pd.DataFrame(comparison_data)

# Calculate improvement percentages
comparison_df['Improvement vs Linear'] = comparison_df['RMSE'].apply(
    lambda x: f"{round((lm_rmse - x) / lm_rmse * 100, 1)}%"
)

# Format RMSE values
comparison_df['RMSE'] = comparison_df['RMSE'].apply(lambda x: f"${x:,.0f}")
comparison_df['R¬≤'] = comparison_df['R¬≤'].apply(lambda x: f"{x:.3f}")

# Display table
print("\nModel Performance Comparison on Test Data")
print("=" * 80)
print(comparison_df.to_string(index=False))
print("=" * 80)
```

**When Simple Beats Complex: The Linear Regression Baseline**

Before embracing the complexity of random forests, we should ask: **does the added sophistication justify the cost?** Linear regression is our baseline ‚Äî it's interpretable, fast, and beloved by business stakeholders who want to understand exactly how features drive predictions.

For this housing dataset, linear regression achieves approximately $42,000 RMSE. This is respectable performance requiring zero hyperparameter tuning and producing coefficients that tell a clear story: "Each additional square foot adds $X to sale price."

**The Ensemble Advantage: When Random Forests Win**

The comparison table reveals three critical insights:

1. **Single trees don't justify complexity:** A 1-tree random forest performs worse than linear regression. If you're only building one tree, stick with linear regression ‚Äî you get similar or better performance plus interpretability.

2. **The 100-tree sweet spot:** Jumping to 100 trees delivers roughly **8-10% improvement over linear regression** (reducing RMSE from ~$42K to ~$38K). This $4,000 per-prediction improvement is meaningful when you're valuing hundreds of properties or when prediction accuracy directly impacts business decisions like automated lending or real estate pricing algorithms.

3. **Diminishing returns beyond 1,000 trees:** Growing from 100 to 1,000 trees adds minimal incremental value ‚Äî perhaps another 1-2% improvement. Unless computational resources are free and you're in a high-stakes domain, 100-500 trees is the pragmatic choice.

**The Practical Decision Framework**

When should you choose random forests over linear regression?

- **Choose Random Forests when:**
  - You have complex, non-linear relationships (housing features interact in complex ways)
  - Prediction accuracy is paramount (algorithmic trading, medical diagnosis)
  - You have ample computational resources
  - Interpretability is less critical than performance
  
- **Choose Linear Regression when:**
  - You need to explain predictions to non-technical stakeholders
  - Speed matters (real-time prediction systems)
  - Your relationships are relatively linear
  - You're operating with limited computational resources
  - The 8-10% accuracy gain doesn't justify the complexity cost

For our housing dataset with its non-linear zip code effects and feature interactions, **the 100-tree random forest wins decisively**. But in domains where relationships are more linear or where explainability trumps accuracy, linear regression remains the professional choice.


